{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Numbers_Logistic R_FeedForward.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMo1CoUU+40dGdsM9Qjsk+K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vbanai/Linear_Logistic_Regression_Projects/blob/main/Numbers_Logistic_R_FeedForward.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcikyuQ0BLgO"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import DataLoader\n",
        "from google.colab import files\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAMOdwfHZOM5"
      },
      "source": [
        "\n",
        "# Hyperparmeters\n",
        "batch_size = 128\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Other constants\n",
        "input_size = 28*28\n",
        "num_classes = 10\n",
        "\n",
        "\n",
        "# Download dataset\n",
        "dataset = MNIST(root='data/', train=True, transform=transforms.ToTensor(), download=True)\n",
        "\n",
        "# Training validation & test dataset\n",
        "train_ds, val_ds = random_split(dataset, [50000, 10000])\n",
        "test_ds = MNIST(root='data/', train=False, transform=transforms.ToTensor())\n",
        "\n",
        "# Dataloaders\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_ds, batch_size*2, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_ds, batch_size*2, num_workers=2, pin_memory=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDIhPm_Qgp4G"
      },
      "source": [
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
        "\n",
        "\n",
        "def evaluate(model, val_loader):\n",
        "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "    return model.validation_epoch_end(outputs)\n",
        "\n",
        "class MnistModel(nn.Module):\n",
        "    \"\"\"Feedfoward neural network with 1 hidden layer\"\"\"\n",
        "    def __init__(self, in_size, out_size):\n",
        "        super().__init__()\n",
        "        # hidden layer\n",
        "        self.linear1 = nn.Linear(in_size, 16)\n",
        "        # hidden layer 2\n",
        "        self.linear2 = nn.Linear(16, 32)\n",
        "        # output layer\n",
        "        self.linear3 = nn.Linear(32, out_size)\n",
        "        \n",
        "    def forward(self, xb):\n",
        "        # Flatten the image tensors\n",
        "        out = xb.view(xb.size(0), -1)\n",
        "        # Get intermediate outputs using hidden layer 1\n",
        "        out = self.linear1(out)\n",
        "        # Apply activation function\n",
        "        out = F.relu(out)\n",
        "        # Get intermediate outputs using hidden layer 2\n",
        "        out = self.linear2(out)\n",
        "        # Apply activation function\n",
        "        out = F.relu(out)\n",
        "        # Get predictions using output layer\n",
        "        out = self.linear3(out)\n",
        "        return out\n",
        "    \n",
        "    def training_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                  # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                    # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
        "        acc = accuracy(out, labels)           # Calculate accuracy\n",
        "        return {'val_loss': loss, 'val_acc': acc}\n",
        "        \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "        batch_accs = [x['val_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
        "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
        "    \n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MR4kWsForPSk"
      },
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "\n",
        "device = get_default_device()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZp8qdT_rboL"
      },
      "source": [
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "        \n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl: \n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)\n",
        "\n",
        "\n",
        "train_loader = DeviceDataLoader(train_loader, device)\n",
        "val_loader = DeviceDataLoader(val_loader, device)\n",
        "test_loader = DeviceDataLoader(test_loader, device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vg94J6kRsEmN"
      },
      "source": [
        "def evaluate(model, val_loader):\n",
        "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "    return model.validation_epoch_end(outputs)\n",
        "\n",
        "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
        "    history = []\n",
        "    optimizer = opt_func(model.parameters(), lr)\n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase \n",
        "        for batch in train_loader:\n",
        "            loss = model.training_step(batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        # Validation phase\n",
        "        result = evaluate(model, val_loader)\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "    return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDkC-GD-sLWk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e24c967-8249-4e8c-b639-447286a9c4c5"
      },
      "source": [
        "model = MnistModel(input_size, out_size=num_classes)\n",
        "to_device(model, device)\n",
        "\n",
        "history = fit(20, 0.05, model, train_loader, val_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [0], val_loss: 0.5320, val_acc: 0.8464\n",
            "Epoch [1], val_loss: 0.3817, val_acc: 0.8863\n",
            "Epoch [2], val_loss: 0.3177, val_acc: 0.9072\n",
            "Epoch [3], val_loss: 0.2824, val_acc: 0.9141\n",
            "Epoch [4], val_loss: 0.2541, val_acc: 0.9245\n",
            "Epoch [5], val_loss: 0.2406, val_acc: 0.9282\n",
            "Epoch [6], val_loss: 0.2337, val_acc: 0.9324\n",
            "Epoch [7], val_loss: 0.2195, val_acc: 0.9349\n",
            "Epoch [8], val_loss: 0.2096, val_acc: 0.9383\n",
            "Epoch [9], val_loss: 0.2008, val_acc: 0.9389\n",
            "Epoch [10], val_loss: 0.1897, val_acc: 0.9429\n",
            "Epoch [11], val_loss: 0.1828, val_acc: 0.9454\n",
            "Epoch [12], val_loss: 0.1797, val_acc: 0.9453\n",
            "Epoch [13], val_loss: 0.1837, val_acc: 0.9426\n",
            "Epoch [14], val_loss: 0.1762, val_acc: 0.9446\n",
            "Epoch [15], val_loss: 0.1735, val_acc: 0.9469\n",
            "Epoch [16], val_loss: 0.1688, val_acc: 0.9491\n",
            "Epoch [17], val_loss: 0.1679, val_acc: 0.9479\n",
            "Epoch [18], val_loss: 0.1660, val_acc: 0.9463\n",
            "Epoch [19], val_loss: 0.1704, val_acc: 0.9474\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mz8782qi1GO9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "2b0b2336-a46c-45dd-ac63-bd3c4e6a71a7"
      },
      "source": [
        "#Graphical presentation of the result\n",
        "\n",
        "accuracy=[i['val_acc'] for i in history]\n",
        "x=[]\n",
        "for i in range(len(history)):\n",
        "  x.append(i+1)\n",
        "\n",
        "\n",
        "plt.plot(x, accuracy)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'accuracy')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxV9Z3/8dcnGxAISUhCWBIIqwIKiBHR4lJcqk7rXpeq1bZTu9mx/bUzY6fza/05M792Ol3s/MbasdbWraLdnRa1YLEVFyAoiew7JBCSQMgCIWT7/P64B7zEC1wkN/cm9/18PPLIuWfJ/eRwOe+c7/ec7zF3R0REpLuUeBcgIiKJSQEhIiIRKSBERCQiBYSIiESkgBARkYjS4l1AT8nPz/eSkpJ4lyEi0qesWLFij7sXRFrWbwKipKSEsrKyeJchItKnmNn2Yy1TE5OIiESkgBARkYgUECIiEpECQkREIlJAiIhIRAoIERGJSAEhIiIR9Zv7IEREToa7s31vC69t3sOh9i7GFQxmQv4QRucOIjXF4l1eQlBAiEjSaGhp47VNe1myqY5XN+6hat/B96yTkZZCSV4m4/OHML5gMOMLQt8n5A8hOzM9DlXHjwJCRPqtQx2dvLW9gSWb6liycQ8VOxtxh6wBacyZkMfdF45n7sR8sgels2XPAbbU7WdL3QE21x1gQ20zi9bW0NH17kPV8gZnhEKjW3iMGZZJemr/a7FXQIhIv+HubKjZz6sb61iyaQ9Lt9RzsL2T1BTjrOIc7r1kEhdMymdGUQ5p3Q7oeUMGcE7JsKPmtXd2UVnfwpa6A2zZEwqPLXUHWLS2hr1lbUfWy0hL4bTCLKaOHMrUUaGv00dkkTUw9mccjS3tNB9qpyg3s8d/tgJCRPq02uZWlmzcE/ratIfa5kMAjC8YzE2lRcydVMCc8cPe18E6PTUlOEsYAhQetayxpZ3NQWhsqGlmza4m/rRmN8+WVR5ZZ2xeZig0woJjxNCBmEXfx3GwrZOqfS1U7muhsv4glfVh0/taaG7t4Oyxufz6c+ef9O93IgoIEelTWto6WLq1/kgorK9pBiA3M50PTMzngkn5zJ1UwOicQTGtIzsznVljcpk1JvfIPHenpukQa6obWbOriTXVTazZ1cQLq3YfWWfY4Ix3AyP4Pig99ciBf0f9uwf/yvqD7Nl/6Kj3HZCWQvGwTIpzB1FakktxbiaTCofE5Hc0dz/xWn1AaWmpazRXkf6ns8tZvauRVzfu4dWNdby1vYG2zi4y0lI4pySXuRMLuGBSPlNHDiUlQa8+am5tZ/3u5iOBsXpXE+trmmnr6HrPuqkpxsjsgRTnZlI8bFDw/d3pgqwBJ3UGciJmtsLdSyMt0xmESB/W2t7J0q317N1/iNQUI8XsqO+pKbw7bUZKSrflZqSkwNi8wQwZkDiHg8r6FpZsCp0hvLZ5Dw0t7QBMGTmUuz5QwtyJ+ZxTMoxBGalxrjQ6WQPTKS0ZRmlYH0d7Zxdb6g6wprqRto6uI0EwMnvge/pH4iVxPhEiEpX6A238eV0ti9bU8NeNdbS0dZ7yz8xIS+HCSQVcdeYILplSSPag3r2cs/FgO29s3nvkaqNte1sAGDF0IJdOKeSCSfmcPyGfgqwBvVpXLKWnpnDaiCxOG5EV71KOSQEh0gdsrtvPojU1LFpbw4rt++jy0MHz+lmjuWRKISV5g+nsctydTnc6u5yuLt6dPjLPu82Djs4ulm/bxwurqlm0tob0VGPuxHyuPHMkl08tJCczo8d/n+bWdsq27ePNrXtZuqWeiqoGuhwyM1KZMz6Pj59XwgWT8pk4fEiPNqfIyVEfhEgC6ujs4q0dDSxaW8OiNTVs2XMAgGmjhnLplEIum1rItFFDe/Tg2dXllFc18MKq3Sx4p5qqfQdJSzHOm5DHVUFY5A15f3/BN7a0s2xbPUu37GXp1npW72qkyyE91ZhRlMN5E/KYOzGfs8bkkpGWGM0ryeJ4fRAKCJEEsf9QB69uqGPh2hoWr6tlX0s76anGnPF5XD61kHlTCmN+Zc5h7s6qnU0sWFXNgneq2b63hRSDOeNDYfGhaSOO29xTf6CNZVv38uaWepZurWfd7ibcQ01ZM4tzmDNuGHPG53HWmNw+04/QXykgRBLUoY5Ofr9yF3+sqOaNzXtp6+wie1A6804fzqVTCrlwcn6v3Gx1PO7OmuomXngndGaxZc8BzGB2yTCuOnMkV5wxghQzlm2tZ2nQZHT40tOB6SnMGpPLuePyOHf8MGYW5zAwXYGQSBQQIgnmYFsnzyzbwSN/3cLuplbG5mVy2ZRCLp1aSOnY3IS5iqW7w3cqL3inmhdWVbOhZv9RyzMzUjl7bC5zxudx7rhhTC/KUZNRgtNlriIJorm1nSfe2M5jS7ay90Abs8cN4zs3TueCSfl9ojPWzI5cefPlyyazqbaZl1bXkGLGnPHDOGN0dr8ckyhZKSBEesG+A2387LWt/Oz1bTS3dnDR5ALumTfxPWP/9DUTh2cxcXjiXqYppyamAWFmVwA/BFKBR939292WjwUeAwqAeuB2d68KWz4UWAP8zt3viWWtIrFQ29TKT17dwtNLd9DS1skV00bwhQ9O5Myi7HiXJnJCMQsIM0sFHgIuA6qA5Wb2vLuvCVvtu8AT7v64mc0DvgXcEbb8X4C/xqpGkViprG/hv/+6mefKqujo7OLqGaP4/AcnMrlQf21L3xHLM4jZwCZ33wJgZvOBawidERw2FfhfwfRi4HeHF5jZ2YSGT3wRiNiBIpJoNtft50eLN/P7lTsxgxvPLuKzF01gbN7geJcmctJiGRCjgcqw11XAud3WKQeuJ9QMdR2QZWZ5wD7ge8DtwKXHegMzuxu4G2DMmDE9VrjIyVq9q5EfLd7MglXVDEhL4Y7zxnL3heMZmd079y2IxEK8O6m/CvyXmd1FqClpJ9AJfB5Y4O5Vx7uyw90fAR6B0GWuMa9Wkoa7s/9QBw0t7TQebKehpZ2Gg21hr9uCee3UNh+ivLKBIQPS+NxFE/jk3HHkv887jkUSSSwDYidQHPa6KJh3hLvvInQGgZkNAW5w9wYzOw+4wMw+DwwBMsxsv7vfF8N6JQmt393Ms8sr2bb3QOigf7CdxuDA39l17L85BqankDMog5zMdLIHpfOVyybz8fNKku6ZxdK/xTIglgOTzGwcoWC4BfhY+Apmlg/Uu3sX8DVCVzTh7reFrXMXUKpwkJ5yqKOTF1ft5qk3t7N82z4y0lKYXDiEnEEZjMwZRM6gdHIy08kZlEF2ZnrwOoPsYH72oHTdDSxJIWYB4e4dZnYP8BKhy1wfc/fVZvYAUObuzwMXA98yMyfUxPSFWNUjUlnfwtNLd/DLskr2HmhjbF4m/3TV6dx4djHDBvf8iKUifZ2G2pB+rbPLeWV9LU+9uZ1XNtRhwKVTCrl9zljmTsxP2CeQifQWDbUhSaeu+RDPlVXyi6U72NlwkOFZA/jivEncOrtYVxaJREkBIf2Gu7N0az1Pvbmdl1bvpr3TOX9CHl//mylcNrVQYwSJnCQFhPR5Ta3t/PatnTz15nY21u5n6MA07phTwm1zxjChYEi8yxPpsxQQ0idt23OAP6+rZfH6WpZuqaets4sZRdl858bpfGT6KD2ERqQHKCCkT2jr6GL5tvpQKKyrPfIIzgkFg7nz/LF8ZMYophflxLlKkf5FASEJq7a5lVfW1/HntbUs2bSH/Yc6yEhNYc6EPD5+3ljmnV7ImLzMeJcp0m8pICRhdHU57+xsPNJ0VFHVCMCIoQP5yIxRzDt9OOdPyGPwAH1sRXqD/qdJXHV1OYvX1/LCqt28sr6OPfsPYQZnFefw1csnM+/0QqaMzOoTT1sT6W8UEBIX7s5Lq2t4cNEG1u1uZujANC46bTjzTi/gosnDdWezSAJQQEivcncWrqnhwUUbWVPdxPj8wTx480w+PH0kabpPQSShKCCkV7g7L6+t5cGXN7BqZxMleZl8/6YZXD1jlIJBJEEpICSm3EN9DA8u2khFVSNjhmXy3Y/O4NqZCgaRRKeAkJhwd17ZUMeDizZSXtlAUe4gvnPDdK6bNVpDXoj0EQoI6VHuzqsb9/CDRRt4e0cDo3MG8e3rz+SGs4sUDCJ9jAJCeoS789qmvfxg0QZWbN/HqOyB/N/rzuTGs4vISFMwiPRFCgg5Je7O65v38uCiDSzfto+R2QP5l2vP4KbSIgakaTwkkb5MASHvS1tHF398ZxePvrqV1buaKBw6gAeumcbN5xQrGET6CQWEnJSGljZ+sWwHj7++jZqmQ0wcPoRvX38m1541Ws9pFulnFBASla17DvCz17byy7IqDrZ3MndiPt++YToXTSrQYztF+ikFhByTu7Nsaz2PLtnKorU1pKUY18wczafmjmPKyKHxLk9EYkwBIe/R3tnFgneq+emSrVRUNZKbmc49H5zIHXPGMnzowHiXJyK9RAEhRzQebGf+sh38/PVtVDe2Mj5/MP967RncMKtIT2gTSUIKCKGyvoWfLtnKc2WVtLR1ct74PP712jP44GnD1b8gksQUEEluza4mbnj4ddo7u7h6xig+OXccZ4zOjndZIpIAFBBJbN+BNj7zVBlDB6Xxq8+eT/EwPb5TRN6lgEhSnV3O381/m5rGQ8z/zByFg4i8hwIiSf3HS+t5deMevn39mcwakxvvckQkAWkUtST0h4pd/Pgvm/nYuWO4ZfaYeJcjIglKAZFk1u1u4u9/WcGsMTl88yNT412OiCSwmAaEmV1hZuvNbJOZ3Rdh+Vgze9nMKszsFTMrCubPNLM3zGx1sOzmWNaZLBpb2vnMkysYMjCNh28/W4PqichxxSwgzCwVeAi4EpgK3Gpm3f9k/S7whLtPBx4AvhXMbwE+7u7TgCuAB80sJ1a1JoPOLufeZ99mV8NBHr5tFoW6I1pETiCWZxCzgU3uvsXd24D5wDXd1pkK/DmYXnx4ubtvcPeNwfQuoBYoiGGt/d4PFm7glfV1fPMj0ygtGRbvckSkD4hlQIwGKsNeVwXzwpUD1wfT1wFZZpYXvoKZzQYygM3d38DM7jazMjMrq6ur67HC+5sXV1XzX4s3cXNpMbedq05pEYlOvDupvwpcZGZvAxcBO4HOwwvNbCTwJPAJd+/qvrG7P+Lupe5eWlCgE4xINtY085XnyplRnMP/uWYaZho6Q0SiE8v7IHYCxWGvi4J5RwTNR9cDmNkQ4AZ3bwheDwX+CHzd3d+MYZ39VlNrO3c/uYJBGan8+PZZeqCPiJyUWJ5BLAcmmdk4M8sAbgGeD1/BzPLN7HANXwMeC+ZnAL8l1IH9qxjW2G91dTlfnr+SyvoWHvrYLEZmD4p3SSLSx8QsINy9A7gHeAlYCzzn7qvN7AEzuzpY7WJgvZltAAqBfwvm3wRcCNxlZiuDr5mxqrU/+uHLG3l5XS3/+8NTOXd83ok3EBHpxtw93jX0iNLSUi8rK4t3GQlh4ZoaPv1EGTfMKuK7H52ufgcROSYzW+HupZGWxbuTWnrYptr9fPnZlZw5Opt/u+4MhYOIvG8KiH6kubWdzzxZRkZaCj++42x1SovIKdForv1EV5fzlefK2ba3hSc/NZvROeqUFpFTozOIfuKhxZv405oa/umqKZw/IT/e5YhIP6CA6AcWr6vl+4s2cO3MUXzyAyXxLkdE+gkFRB/3yvpavvjM20wZMZRvXa8rlkSk5ygg+ih359FXt/DJny+nKHcQj95ZyqAMdUqLSM9RJ3Uf1Nreydd/u4pfv1XFFdNG8L2bZjB4gP4pRaRn6ajSx9Q2tfKZp1bw9o4G7r1kEvdeMomUFDUriUjPU0D0IRVVDdz9xAoaD7bzo9tmcdWZI+Ndkoj0YwqIPuL58l38/S/LyR8ygF997jymjcqOd0ki0s8pIBJcV5fzvYXreWjxZs4pyeXh288mf8iAeJclIklAAZHA9h/q4EvzV7JobQ23nFPMA9ecQUaaLjwTkd6hgEhQO/a28LdPLGdz3QHu/8hU7jy/RPc4iEivUkAkoNc37+HzT7+FOzz+idnMnaShM0Sk9ykgEoi789Sb27n/f9YwLn8wj368lJL8wfEuS0SSlAIiQbR1dHH//6zmF0t3MO/04fzwlplkDUyPd1kiksSiCggz+w3wU+AFd++KbUnJZ+/+Q3z+6bdYurWez140gb//0Gmk6uY3EYmzaC+J+RHwMWCjmX3bzE6LYU1JpbW9k+sffp23Kxt48OaZ3Hfl6QoHEUkIUQWEuy9y99uAWcA2YJGZvW5mnzAztYOcggXvVLN9bwsP3zaLa88aHe9yRESOiPqiejPLA+4C/hZ4G/ghocBYGJPKksQzy3ZQkpfJvNOHx7sUEZGjRNsH8VvgNOBJ4CPuXh0setbMymJVXH+3saaZ5dv28bUrT9c9DiKScKK9iuk/3X1xpAXuXtqD9SSVZ5ZVkp5q3HB2UbxLERF5j2ibmKaaWc7hF2aWa2afj1FNSaG1vZNfv1XF5dNGaGwlEUlI0QbEp9294fALd98HfDo2JSWHF1ftpvFgOx+bPSbepYiIRBRtQKRaWCO5maUCGbEpKTn8YtkOxgzL5LzxefEuRUQkomgD4kVCHdKXmNklwDPBPHkfNtXuZ9nWem6ZXaynwYlIwoq2k/ofgc8AnwteLwQejUlFSWD+sh2kpRg3qnNaRBJYVAERDK/xcPAlp+Bw5/RlUwsZnjUw3uWIiBxTtPdBTAK+BUwFjhzV3H18jOrqt15avZt9Le3cqs5pEUlw0fZB/IzQ2UMH8EHgCeCpE21kZleY2Xoz22Rm90VYPtbMXjazCjN7xcyKwpbdaWYbg687o6wz4c1fVklR7iDmTtQzHkQksUUbEIPc/WXA3H27u98P/M3xNgiudHoIuJLQmcetZja122rfBZ5w9+nAA4TOUjCzYcA3gXOB2cA3zSw3yloT1pa6/byxZS+3zh6jzmkRSXjRBsQhM0shNJrrPWZ2HTDkBNvMBja5+xZ3bwPmA9d0W2cq8OdgenHY8g8BC929PrjnYiFwRZS1Jqxnl1eSmmJ8VJ3TItIHRBsQ9wKZwN8BZwO3Aydq9hkNVIa9rgrmhSsHrg+mrwOygkEBo9kWM7vbzMrMrKyuri7KXyU+DnV08ssVVVw6ZTjDh6pzWkQS3wkDImgqutnd97t7lbt/wt1vcPc3e+D9vwpcZGZvAxcBO4HOaDd290fcvdTdSwsKCnqgnNhZuKaG+gNt6pwWkT7jhFcxuXunmc19Hz97J1Ac9roomBf+s3cRnEGY2RDgBndvMLOdwMXdtn3lfdSQMJ5ZtoPROYO4YFJiB5mIyGHRNjG9bWbPm9kdZnb94a8TbLMcmGRm48wsA7gFeD58BTPLD/o2AL4GPBZMvwRcHgwKmAtcHszrk7btOcBrm/ZyyznFelqciPQZ0d5JPRDYC8wLm+fAb461gbt3mNk9hA7sqcBj7r7azB4Aytz9eUJnCd8yMwf+Cnwh2LbezP6FUMgAPODu9dH/Woll/uHO6dLiE68sIpIgzN3jXUOPKC0t9bKyxHt2UVtHF+d/+2XOGpPLTz6uR2eISGIxsxXHeq5PtHdS/4zQGcNR3P2Tp1hbv7dobQ179rdpWG8R6XOibWL6Q9j0QEKXpO7q+XL6n2eW7WBU9kAunKzOaRHpW6IdrO/X4a/N7BlgSUwq6kd27G3h1Y17+PKlk9U5LSJ9TrRXMXU3CRjek4X0R/OX7yDF4KZzdOe0iPQ90fZBNHN0H8RuQs+IkGNo7+ziubIq5p0+nJHZg+JdjojISYu2iSkr1oX0Ny+vrWHP/kO6c1pE+qyompjM7Dozyw57nWNm18aurL7vF8sqGZk9kIvUOS0ifVS0fRDfdPfGwy/cvYHQcNwSQWV9C69urOOm0mLSUt9vN4+ISHxFe/SKtF60l8gmnWeXV2LATefozmkR6buiDYgyM/u+mU0Ivr4PrIhlYX1VqHO6kotPG87oHHVOi0jfFW1AfBFoA54l9OCfVoJxk+Rof15XS22zOqdFpO+L9iqmA8B7nikt7/XMsh0UDh3AB09T57SI9G3RXsW00Mxywl7nmlmfHX47Vqr2tfCXDXXcrM5pEekHoj2K5QdXLgEQPCdad1J389zy0FNS1TktIv1BtAHRZWZHGtXNrIQIo7sms47OLp4tq+SiyQUU5WbGuxwRkVMW7aWqXweWmNlfAAMuAO6OWVV90OL1ddQ0HeKBa9Q5LSL9Q7Sd1C+aWSmhUHgb+B1wMJaF9TXPLNvB8KwBzDtdLW8i0j9EO1jf3wL3AkXASmAO8AZHP4I0ae1qOMgr62v5/MUTSVfntIj0E9Eeze4FzgG2u/sHgbOAhuNvkjyeK6vEgZvVOS0i/Ui0AdHq7q0AZjbA3dcBp8WurL6js8t5dnklF0wqoHiYOqdFpP+INiCqgvsgfgcsNLPfA9tjV1bfsal2P9WNrVw9Y1S8SxER6VHRdlJfF0zeb2aLgWzgxZhV1YeUV4Za2mYW55xgTRGRvuWkR2R197/EopC+qryqgawBaYzPHxzvUkREepQuuTlFFVWNnFmUTUqKxbsUEZEepYA4Ba3tnaytbmJ6kZqXRKT/UUCcgrXVTXR0OTOLs0+8sohIH6OAOAUVVaGnsOoMQkT6IwXEKSivbCB/yABGZg+MdykiIj1OAXEKyqsamFmcjZk6qEWk/4lpQJjZFWa23sw2mdl7nkhnZmPMbLGZvW1mFWZ2VTA/3cweN7N3zGytmX0tlnW+H02t7WyuO6DmJRHpt2IWEGaWCjwEXAlMBW41s6ndVvtn4Dl3Pwu4BfhRMP+jwAB3PxM4G/hM8AyKhLEq6H+YoRvkRKSfiuUZxGxgk7tvcfc2YD5wTbd1HBgaTGcDu8LmDzazNGAQ0AY0xbDWk1Z+uIN6tK5gEpH+KZYBMRqoDHtdFcwLdz9wu5lVAQuALwbzfwUcAKqBHcB33b2++xuY2d1mVmZmZXV1dT1c/vGVVzYwZlgmuYMzevV9RUR6S7w7qW8Ffu7uRcBVwJNmlkLo7KMTGAWMA75iZuO7b+zuj7h7qbuXFhQU9GbdVFQ1qHlJRPq1WAbETiD8AQlFwbxwnwKeA3D3N4CBQD7wMeBFd29391rgNaA0hrWelNrmVnY1tjKjSM1LItJ/xTIglgOTzGycmWUQ6oR+vts6O4BLAMxsCqGAqAvmzwvmDyb0BLt1Maz1pFRU6gY5Een/YhYQ7t4B3AO8BKwldLXSajN7wMyuDlb7CvBpMysHngHucncndPXTEDNbTShofubuFbGq9WRVVDWQYnDG6KEnXllEpI866eG+T4a7LyDU+Rw+7xth02uAD0TYbj+hS10TUnlVI5MLs8jMiOnuExGJq3h3Uvc57k55VQPT1f8gIv2cAuIkVdYfpKGlXVcwiUi/p4A4SeVVoUeMzlAHtYj0cwqIk1Re2UBGWgqnjciKdykiIjGlgDhJFVWNTBs1lPRU7ToR6d90lDsJHZ1dvLOzUc1LIpIUFBAnYVPdfg62dzJDjxgVkSSggDgJuoNaRJKJAuIkrKxqIGtAGuPyBse7FBGRmFNAnISKqgamF2eTkqJHjIpI/6eAiFJreyfrqpvVvCQiSUMBEaU11U10dLmG+BaRpKGAiFJFZXAHtYbYEJEkoYCIUkVVIwVZAxgxdGC8SxER6RUKiCitrGpgRlEOZuqgFpHkoICIQlNrO1vqDqj/QUSSigIiCu9UBTfIqf9BRJKIAiIK7w7xrTMIEUkeCogoVFQ2MjYvk5zMjHiXIiLSaxQQUQg9YlTNSyKSXBQQJ1Db3Ep1Y6ual0Qk6SggTuDwCK66QU5Eko0C4gTKqxpIMZg2ami8SxER6VUKiBMor2pkcmEWmRlp8S5FRKRXKSCOw92pCO6gFhFJNgqI49hR30JDS7v6H0QkKSkgjqP88B3UuoJJRJKQAuI4yisbGJCWwmkjsuJdiohIr1NAHEdFVQPTRg0lPVW7SUSSj458x9DR2cWqnU26g1pEklZMA8LMrjCz9Wa2yczui7B8jJktNrO3zazCzK4KWzbdzN4ws9Vm9o6Z9eqTejbW7udgeyczitX/ICLJKWYX95tZKvAQcBlQBSw3s+fdfU3Yav8MPOfuD5vZVGABUGJmacBTwB3uXm5meUB7rGqNpOLICK46gxCR5BTLM4jZwCZ33+LubcB84Jpu6zhw+BblbGBXMH05UOHu5QDuvtfdO2NY63uUVzWSNTCNkrzBvfm2IiIJI5YBMRqoDHtdFcwLdz9wu5lVETp7+GIwfzLgZvaSmb1lZv8Q6Q3M7G4zKzOzsrq6uh4tvryygelF2aSk6BGjIpKc4t1JfSvwc3cvAq4CnjSzFEJNX3OB24Lv15nZJd03dvdH3L3U3UsLCgp6rKjW9k7W725W85KIJLVYBsROoDjsdVEwL9yngOcA3P0NYCCQT+hs46/uvsfdWwidXcyKYa1HWVPdREeX6womEUlqsQyI5cAkMxtnZhnALcDz3dbZAVwCYGZTCAVEHfAScKaZZQYd1hcBa+gl5ZWhDuqZGmJDRJJYzK5icvcOM7uH0ME+FXjM3Veb2QNAmbs/D3wF+ImZfZlQh/Vd7u7APjP7PqGQcWCBu/8xVrV2V1HVyPCsAYzI7tUra0VEEkpMx7B29wWEmofC530jbHoN8IFjbPsUoUtde50eMSoiEv9O6oTTeLCdLXUHmKkb5EQkySkgulm18/AIrjqDEJHkpoDoZmXQQa0hvkUk2SkguqmoaqAkL5OczIx4lyIiElcKiG4qqhrVvCQiggLiKLVNrVQ3tqp5SUQEBcRRDj9iVDfIiYgoII5SUdVAaooxbZTOIEREFBBhVlY2MLkwi0EZqfEuRUQk7hQQAXfnnZ2NzFD/g4gIoIA4Ykd9Cw0t7bqCSUQkoIAIHL5BTs+gFhEJUUAEKqoaGZCWwuTCrHiXIiKSEBQQgfLKBqaNGkp6qnaJiAgoIADo6Oxi1a5GZuj+BxGRIxQQwMba/bS2d+kZ1CIiYRQQvPuIUQ2xISLyLgUEoSE2hg5MoyRvcLxLERFJGAoIQkNsTC/KISXF4l2KiEjCSPqAaG3vZN3uZt3/ICLSTdIHRHNrBx+ePmHwZHIAAAa6SURBVJLzJ+THuxQRkYSSFu8C4q0gawA/vOWseJchIpJwkv4MQkREIlNAiIhIRAoIERGJSAEhIiIRKSBERCQiBYSIiESkgBARkYgUECIiEpG5e7xr6BFmVgdsj3cdx5EP7Il3Eceh+k6N6js1qu/UnEp9Y929INKCfhMQic7Myty9NN51HIvqOzWq79SovlMTq/rUxCQiIhEpIEREJCIFRO95JN4FnIDqOzWq79SovlMTk/rUByEiIhHpDEJERCJSQIiISEQKiB5iZsVmttjM1pjZajO7N8I6F5tZo5mtDL6+EYc6t5nZO8H7l0VYbmb2n2a2ycwqzGxWL9Z2Wti+WWlmTWb2pW7r9Oo+NLPHzKzWzFaFzRtmZgvNbGPwPfcY294ZrLPRzO7sxfr+w8zWBf9+vzWznGNse9zPQgzru9/Mdob9G151jG2vMLP1wWfxvl6s79mw2raZ2cpjbNsb+y/icaXXPoPurq8e+AJGArOC6SxgAzC12zoXA3+Ic53bgPzjLL8KeAEwYA6wNE51pgK7Cd3EE7d9CFwIzAJWhc37DnBfMH0f8O8RthsGbAm+5wbTub1U3+VAWjD975Hqi+azEMP67ge+GsW//2ZgPJABlHf//xSr+rot/x7wjTjuv4jHld76DOoMooe4e7W7vxVMNwNrgdHxrep9uQZ4wkPeBHLMbGQc6rgE2Ozucb073t3/CtR3m30N8Hgw/ThwbYRNPwQsdPd6d98HLASu6I363P1P7t4RvHwTKOrp943WMfZfNGYDm9x9i7u3AfMJ7fcedbz6zMyAm4Bnevp9o3Wc40qvfAYVEDFgZiXAWcDSCIvPM7NyM3vBzKb1amEhDvzJzFaY2d0Rlo8GKsNeVxGfoLuFY//HjPc+LHT36mB6N1AYYZ1E2Y+fJHRGGMmJPguxdE/QBPbYMZpHEmH/XQDUuPvGYyzv1f3X7bjSK59BBUQPM7MhwK+BL7l7U7fFbxFqMpkB/D/gd71dHzDX3WcBVwJfMLML41DDcZlZBnA18MsIixNhHx7hoXP5hLxW3My+DnQATx9jlXh9Fh4GJgAzgWpCzTiJ6FaOf/bQa/vveMeVWH4GFRA9yMzSCf0jPu3uv+m+3N2b3H1/ML0ASDez/N6s0d13Bt9rgd8SOpUPtxMoDntdFMzrTVcCb7l7TfcFibAPgZrDzW7B99oI68R1P5rZXcCHgduCA8h7RPFZiAl3r3H3TnfvAn5yjPeN9/5LA64Hnj3WOr21/45xXOmVz6ACoocE7ZU/Bda6+/ePsc6IYD3MbDah/b+3F2scbGZZh6cJdWau6rba88DHg6uZ5gCNYaeyveWYf7nFex8GngcOXxFyJ/D7COu8BFxuZrlBE8rlwbyYM7MrgH8Arnb3lmOsE81nIVb1hfdpXXeM910OTDKzccEZ5S2E9ntvuRRY5+5VkRb21v47znGldz6DseyBT6YvYC6h07wKYGXwdRXwWeCzwTr3AKsJXZHxJnB+L9c4Pnjv8qCOrwfzw2s04CFCV5C8A5T2co2DCR3ws8PmxW0fEgqqaqCdUBvup4A84GVgI7AIGBasWwo8GrbtJ4FNwdcnerG+TYTang9/Dn8crDsKWHC8z0Iv1fdk8NmqIHSgG9m9vuD1VYSu2tncm/UF839++DMXtm489t+xjiu98hnUUBsiIhKRmphERCQiBYSIiESkgBARkYgUECIiEpECQkREIlJAiCQAC41S+4d41yESTgEhIiIRKSBEToKZ3W5my4JnAPy3maWa2X4z+0EwXv/LZlYQrDvTzN60d5/LkBvMn2hmi4IBB98yswnBjx9iZr+y0LMcnj58x7hIvCggRKJkZlOAm4EPuPtMoBO4jdDd32XuPg34C/DNYJMngH909+mE7hw+PP9p4CEPDTh4PqE7eSE0UueXCI33Px74QMx/KZHjSIt3ASJ9yCXA2cDy4I/7QYQGSevi3UHdngJ+Y2bZQI67/yWY/zjwy2D8ntHu/lsAd28FCH7eMg/G/gmeYlYCLIn9ryUSmQJCJHoGPO7uXztqptn/7rbe+x2/5lDYdCf6/ylxpiYmkei9DNxoZsPhyHOBxxL6f3RjsM7HgCXu3gjsM7MLgvl3AH/x0FPBqszs2uBnDDCzzF79LUSipL9QRKLk7mvM7J8JPUUshdAIoF8ADgCzg2W1hPopIDQM84+DANgCfCKYfwfw32b2QPAzPtqLv4ZI1DSaq8gpMrP97j4k3nWI9DQ1MYmISEQ6gxARkYh0BiEiIhEpIEREJCIFhIiIRKSAEBGRiBQQIiIS0f8HBcFFRcNXjnAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCptIN1Eee0i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a0d8c5c-555a-4a2c-a2f8-0bc73c3f783c"
      },
      "source": [
        "#Accuracy of the testset\n",
        "\n",
        "f=[]\n",
        "\n",
        "for img, label in test_loader:\n",
        "  outputs = model(img)\n",
        "  _, preds  = torch.max(outputs, dim=1)\n",
        "  t=torch.tensor(torch.sum(preds == label).item() / len(preds))\n",
        "  f.append(t)\n",
        "\n",
        "\n",
        "Sum=sum(f)\n",
        "print(Sum/len(f))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.9517)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ert9-tIa13J3"
      },
      "source": [
        "torch.save(model.state_dict(), 'mnst.logistic.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chuo9kQF1841",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "615216df-819b-4df5-d0a3-36903efc0951"
      },
      "source": [
        "#Trying out the saved model\n",
        "\n",
        "model2=MnistModel(input_size, out_size=num_classes)\n",
        "model2.load_state_dict(torch.load('mnst.logistic.pth'))\n",
        "model2.state_dict\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "f=[]\n",
        "\n",
        "for img, label in test_ds:\n",
        "  outputs = model2(img)\n",
        "  _, preds  = torch.max(outputs, dim=1)\n",
        "  t=torch.tensor(torch.sum(preds == label).item() / len(preds))\n",
        "  f.append(t)\n",
        "\n",
        "\n",
        "Sum=sum(f)\n",
        "print(Sum/len(f))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.9518)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6of5u1I4A6_"
      },
      "source": [
        "# with some individual images:\n",
        "\n",
        "def predict_image(img, model2):\n",
        "    xb = img.unsqueeze(0)\n",
        "    yb = model(xb)\n",
        "    print(yb)\n",
        "    _, preds  = torch.max(yb, dim=1)\n",
        "    return preds[0].item()\n",
        "\n",
        "img, label = test_ds[46]\n",
        "plt.imshow(img[0], cmap='gray')\n",
        "print('Label:', label, ', Predicted:', predict_image(img, model2))\n",
        "\n",
        "print(img[0])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpRyCEVGlYvP"
      },
      "source": [
        "\n",
        "from IPython.display import display, Javascript, Image\n",
        "import cv2\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDC4n4IDlk41"
      },
      "source": [
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "\n",
        "  # get photo data\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  # get OpenCV format image\n",
        "  img = js_to_image(data) \n",
        "  # save image\n",
        "  cv2.imwrite(filename, img)\n",
        "\n",
        "  return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "PeZ3hzy0lpuz",
        "outputId": "7ff72e11-123e-46c3-aef4-9d276b63523d"
      },
      "source": [
        "x=take_photo()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function takePhoto(quality) {\n",
              "      const div = document.createElement('div');\n",
              "      const capture = document.createElement('button');\n",
              "      capture.textContent = 'Capture';\n",
              "      div.appendChild(capture);\n",
              "\n",
              "      const video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
              "\n",
              "      document.body.appendChild(div);\n",
              "      div.appendChild(video);\n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      // Resize the output to fit the video element.\n",
              "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
              "\n",
              "      // Wait for Capture to be clicked.\n",
              "      await new Promise((resolve) => capture.onclick = resolve);\n",
              "\n",
              "      const canvas = document.createElement('canvas');\n",
              "      canvas.width = video.videoWidth;\n",
              "      canvas.height = video.videoHeight;\n",
              "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "      stream.getVideoTracks()[0].stop();\n",
              "      div.remove();\n",
              "      return canvas.toDataURL('image/jpeg', quality);\n",
              "    }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJDJ8lF9rjYP"
      },
      "source": [
        "img_array_test=cv2.imread(\"/content/photo.jpg\")\n",
        "new_img_array_test=cv2.resize(img_array_test,(64,64))\n",
        "new_img_array_test2=new_img_array_test/255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfyLeGtbr29M",
        "outputId": "4bdacbf6-191f-4296-c63a-f91088bab0b9"
      },
      "source": [
        "inverse=1 - new_img_array_test2\n",
        "inverse_test = np.array(inverse)\n",
        "inverse_test_float=inverse_test.astype('float32')\n",
        "inverse_test_tensor=torch.from_numpy(inverse_test_float)\n",
        "\n",
        "print(predict_image(img, model2))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-6.8274,  6.4903, -0.3940,  3.3753, -1.7823, -1.7014, -4.1278,  1.2011,\n",
            "          1.6189, -1.3595]], grad_fn=<AddmmBackward>)\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29ToimqnnuL9"
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "\n",
        "plt.imshow(imgResized, cmap='gray')\n",
        "Gray=cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "Gray1=cv2.resize(Gray, (28, 28))\n",
        "\n",
        "Gray2=torch.tensor(Gray1)\n",
        "Gray3 = Gray2.type(torch.float32)\n",
        "Gray4 = Gray3.unsqueeze(0)\n",
        "Gray5=tf.keras.utils.normalize(Gray4, axis=1)\n",
        "\n",
        "print(Gray5.shape)\n",
        "print(Gray5.dtype)\n",
        "print(Gray5)\n",
        "\n",
        "print('Predicted:', predict_image(Gray5, model2))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_62FW3xYxwCu"
      },
      "source": [
        "uploaded=files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I15HfB7pyYRq"
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "y=cv2.imread('3 (11).jpg')\n",
        "#picture=resize(y, (28,28), mode='constanst', antialiasing=True)\n",
        "y1=cv2.resize(y, (28, 28))\n",
        "y2=cv2.cvtColor(y1, cv2.COLOR_BGR2GRAY)\n",
        "y3=torch.tensor(y2)\n",
        "y4 = y3.type(torch.float32)\n",
        "y5 = y4.unsqueeze(0)\n",
        "y6=tf.keras.utils.normalize(y5, axis=1)\n",
        "print(y6.shape)\n",
        "print(y6.dtype)\n",
        "print(y6)\n",
        "\n",
        "\n",
        "#print('Predicted:', predict_image(y6, model2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSdpjSJwwFxk",
        "outputId": "1ca14372-6012-42d9-ff2a-aacdf0a07abe"
      },
      "source": [
        "print('Predicted:', predict_image(y6, model2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-1.5928,  0.6344,  1.2368,  6.0241, -1.6542,  1.7492, -3.2911, -2.0907,\n",
            "          2.0241, -1.4299]], grad_fn=<AddmmBackward>)\n",
            "Predicted: 3\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}